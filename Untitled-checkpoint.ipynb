{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6ce15c-7fd1-482d-9fdd-42c34a534185",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe563f9-8e64-459c-b949-f5f6c398bfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "# OpenAI API 키 설정\n",
    "openai_api_key = \"sk-proj-Uxu_7i4Ahk0VlN-0z05QDYhF85RTJkr09-CSBvYaCXQyhdcxXeIuDIQ4HzRJFiwpTQAfz1njAyT3BlbkFJW_jzBN3EFz9di7_2ojtRWdywntwR4UxpN9TVVszIR6OiKkmr5NhVNvu_izcRhUsMBAhIVWPLYAsk-proj-Uxu_7i4Ahk0VlN-0z05QDYhF85RTJkr09-CSBvYaCXQyhdcxXeIuDIQ4HzRJFiwpTQAfz1njAyT3BlbkFJW_jzBN3EFz9di7_2ojtRWdywntwR4UxpN9TVVszIR6OiKkmr5NhVNvu_izcRhUsMBAhIVWPLYA\"\n",
    "\n",
    "# OpenAI 임베딩 모델 초기화\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    openai_api_key=openai_api_key\n",
    ")\n",
    "\n",
    "# 텍스트를 벡터로 변환\n",
    "text = \"OpenAI creates amazing AI models!\"\n",
    "vector = embeddings.embed_query(text)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"Embedding Vector:\", vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9707d5b-046d-4032-b0e9-e1c9af4f9e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# PDF 파일 경로 지정\n",
    "pdf_path = \"example.pdf\"\n",
    "\n",
    "# PDF 파일 로드\n",
    "loader = PyPDFLoader(\"C:/Users/Lenovo/Downloads/초거대언어모델 연구 동향 (1).pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "# 로드된 문서 출력\n",
    "for doc in documents:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e03f6dc-c745-468c-b74b-54de4d43530f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# PDF 파일 경로\n",
    "pdf_path = r\"C:/Users/Lenovo/Downloads/초거대언어모델 연구 동향 (1).pdf\"\n",
    "\n",
    "# PDF 파일 로더 생성\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "\n",
    "# 문서 로드\n",
    "docs = loader.load()\n",
    "\n",
    "# 텍스트 분할기 생성\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",  # 문단 단위로 분리\n",
    "    chunk_size=100,    # 각 분할 크기\n",
    "    chunk_overlap=20,  # 겹칠 부분 크기\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "# 텍스트 분할\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# 분할된 텍스트 출력\n",
    "for i, split in enumerate(splits):\n",
    "    print(f\"Split {i + 1}:\\n{split.page_content}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f7855b-a4d3-4939-857d-5c30eb2f53ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "# OpenAI API 키 설정\n",
    "openai_api_key = \"sk-proj-Uxu_7i4Ahk0VlN-0z05QDYhF85RTJkr09-CSBvYaCXQyhdcxXeIuDIQ4HzRJFiwpTQAfz1njAyT3BlbkFJW_jzBN3EFz9di7_2ojtRWdywntwR4UxpN9TVVszIR6OiKkmr5NhVNvu_izcRhUsMBAhIVWPLYAsk-proj-Uxu_7i4Ahk0VlN-0z05QDYhF85RTJkr09-CSBvYaCXQyhdcxXeIuDIQ4HzRJFiwpTQAfz1njAyT3BlbkFJW_jzBN3EFz9di7_2ojtRWdywntwR4UxpN9TVVszIR6OiKkmr5NhVNvu_izcRhUsMBAhIVWPLYA\"\n",
    "\n",
    "# OpenAI 임베딩 모델 초기화\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    openai_api_key=openai_api_key\n",
    ")\n",
    "\n",
    "# 벡터로 변환할 텍스트\n",
    "text = \"OpenAI creates amazing AI models!\"\n",
    "\n",
    "# 텍스트를 벡터로 변환\n",
    "vector = embeddings.embed_query(text)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"Embedding Vector:\")\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f65bbf-a71f-434c-9dc4-fe2ae45fa8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a193d011-cd1e-48e5-a5a1-6a72a1fdf437",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# 프롬프트 템플릿 정의\n",
    "contextual_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer the question using only the following context.\"),\n",
    "    (\"user\", \"Context: {context}\\\\n\\\\nQuestion: {question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1266fe-a0d9-4e2c-9bf8-aeef9b84986d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# FAISS 벡터 스토어를 Retriever로 변환\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\", \n",
    "    search_kwargs={\"k\": 1}  # 가장 유사한 문서 1개 반환\n",
    ")\n",
    "\n",
    "# OpenAI LLM 설정 (임의의 LLM 사용 가능)\n",
    "llm = OpenAI(\n",
    "    temperature=0,\n",
    "    openai_api_key=openai_api_key\n",
    ")\n",
    "\n",
    "# RAG 체인 생성\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True  # 원본 문서를 반환하도록 설정\n",
    ")\n",
    "\n",
    "# 질문 예제\n",
    "query = \"Where is the Eiffel Tower located?\"\n",
    "result = qa_chain.run(query)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"Answer:\", result['result'])\n",
    "print(\"\\nSource Document:\", result['source_documents'][0].page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0394aa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# 프롬프트 템플릿 정의\n",
    "contextual_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer the question using only the following context.\"),\n",
    "    (\"user\", \"Context: {context}\\n\\nQuestion: {question}\")\n",
    "])\n",
    "\n",
    "# 예제 입력 값\n",
    "context = \"The Eiffel Tower is located in Paris, France.\"\n",
    "question = \"Where is the Eiffel Tower located?\"\n",
    "\n",
    "# 템플릿 렌더링\n",
    "rendered_prompt = contextual_prompt.format_prompt(context=context, question=question).to_string()\n",
    "\n",
    "# 결과 출력\n",
    "print(rendered_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0514e762",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Retriever를 invoke() 메서드로 래핑하는 클래스 정의\n",
    "class RetrieverWrapper:\n",
    "    def __init__(self, retriever):\n",
    "        self.retriever = retriever\n",
    "\n",
    "    def invoke(self, inputs):\n",
    "        # 입력이 딕셔너리인 경우 질문을 추출\n",
    "        if isinstance(inputs, dict):\n",
    "            query = inputs.get(\"question\", \"\")\n",
    "        else:\n",
    "            query = inputs\n",
    "        # Retriever에서 관련 문서 검색\n",
    "        response_docs = self.retriever.get_relevant_documents(query)\n",
    "        return response_docs\n",
    "\n",
    "# Context와 Question을 프롬프트에 연결하는 클래스 정의\n",
    "class ContextToPrompt:\n",
    "    def __init__(self, prompt_template):\n",
    "        self.prompt_template = prompt_template\n",
    "\n",
    "    def invoke(self, inputs):\n",
    "        # 검색된 문서를 텍스트로 변환\n",
    "        if isinstance(inputs, list):\n",
    "            context_text = \"\\n\".join([doc.page_content for doc in inputs])\n",
    "        else:\n",
    "            context_text = inputs\n",
    "\n",
    "        # 프롬프트 템플릿에 context와 question 적용\n",
    "        formatted_prompt = self.prompt_template.format_prompt(\n",
    "            context=context_text,\n",
    "            question=inputs.get(\"question\", \"\")\n",
    "        ).to_string()\n",
    "        return formatted_prompt\n",
    "\n",
    "# LLM 체인 연결\n",
    "llm_chain = LLMChain(llm=model, prompt=contextual_prompt)\n",
    "\n",
    "# RAG 체인 설정\n",
    "rag_chain = {\n",
    "    \"retriever\": RetrieverWrapper(retriever),\n",
    "    \"context_to_prompt\": ContextToPrompt(contextual_prompt),\n",
    "    \"llm_chain\": llm_chain,\n",
    "}\n",
    "\n",
    "# RAG 체인 실행 함수\n",
    "def execute_rag_chain(chain, question):\n",
    "    # Step 1: Retrieve context\n",
    "    retrieved_context = chain[\"retriever\"].invoke({\"question\": question})\n",
    "    \n",
    "    # Step 2: Format context and question into a prompt\n",
    "    formatted_prompt = chain[\"context_to_prompt\"].invoke(retrieved_context)\n",
    "    \n",
    "    # Step 3: Generate a response using the LLM\n",
    "    response = chain[\"llm_chain\"].run({\"context\": formatted_prompt, \"question\": question})\n",
    "    return response\n",
    "\n",
    "# 예제 질문 실행\n",
    "question = \"What is the Eiffel Tower?\"\n",
    "result = execute_rag_chain(rag_chain, question)\n",
    "\n",
    "# 결과 출력\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
